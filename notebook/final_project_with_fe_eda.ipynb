{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd70b78",
   "metadata": {},
   "source": [
    "# üìä Final Project - Customer Churn Prediction for Sendo Farm\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "Sendo Farm is an online grocery e-commerce platform that delivers essential goods, similar to a supermarket, directly to consumers for their daily meals.  \n",
    "Because the products are groceries‚Äîespecially fresh items such as meat, fish, fruits, and vegetables‚Äîcustomer satisfaction is highly sensitive to quality.  \n",
    "\n",
    "Some customers who are dissatisfied with product quality or after-sales service choose to file complaints, request refunds, or report missing items. However, many others leave silently without expressing dissatisfaction and never purchase from Sendo Farm again.  \n",
    "\n",
    "Customer churn is costly, especially in the grocery business where customers make frequent and recurring purchases.  \n",
    "Thus, the goal of this project is to build a **supervised machine learning model** that predicts which customers are at risk of churning. This will allow Sendo Farm to proactively take preventive actions, such as personalized campaigns, compensation, or loyalty offers, to improve customer experience and retention.  \n",
    "\n",
    "### Dataset Description\n",
    "The available data includes:\n",
    "- **Transaction history** (order frequency, order value, recency of last purchase).\n",
    "- **Complaint & refund history** (number of complaints, refund requests).\n",
    "- **Missing items history** (number of orders with missing products).\n",
    "- **Purchase ratio of dry vs fresh goods** (stability and sensitivity to product type).\n",
    "- **Order rating** (order satisfaction)      \n",
    "\n",
    "These features will be engineered into customer-level data suitable for supervised ML classification (churn vs non-churn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179ee42",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1912d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (replace with actual path)\n",
    "# Orders (l·ªãch s·ª≠ giao d·ªãch mua h√†ng)\n",
    "# Customers (th√¥ng tin kh√°ch h√†ng)\n",
    "# Products (SKU, danh m·ª•c, gi√°)\n",
    "# Incidents (s·ª± ki·ªán ph√†n n√†n, y√™u c·∫ßu ho√†n ti·ªÅn, thi·∫øu h√†ng)\n",
    "# Order_rating (ƒë√°nh gi√° ƒë∆°n h√†ng)\n",
    "data = pd.read_csv(\"data/customer_churn.csv\")\n",
    "\n",
    "# Preview dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6921c0",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "- Check missing values  \n",
    "- Target distribution (churn vs not churn)  \n",
    "- Descriptive statistics  \n",
    "- Correlation matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed832a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Distribution plots\n",
    "num_cols = [\"order_count\", \"avg_order_value\", \"complaint_count\", \"refund_count\", \"missing_items_ratio\"]\n",
    "df[num_cols].hist(bins=20, figsize=(12,8))\n",
    "plt.show()\n",
    "\n",
    "# Churn rate\n",
    "df[\"churn\"].value_counts(normalize=True).plot(kind=\"bar\", title=\"Churn Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df[num_cols + [\"churn\"]].corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.show()\n",
    "\n",
    "# Compare churn vs non-churn customers\n",
    "sns.boxplot(x=\"churn\", y=\"order_count\", data=df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045b29b",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Preprocessing\n",
    "- Encode categorical variables  \n",
    "- Scale numerical features  \n",
    "- Train/test split  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## === Feature Engineering for Frequency Change ===\n",
    "# Important: sudden drop in order frequency often signals churn risk\n",
    "\n",
    "# Average frequency in month -2 and -3\n",
    "# (Assume df has a 'order_date' column for transaction history)\n",
    "df[\"order_month\"] = pd.to_datetime(df[\"order_date\"]).dt.to_period(\"M\")\n",
    "\n",
    "# Count orders per customer per month\n",
    "monthly_orders = df.groupby([\"customer_id\", \"order_month\"]).size().reset_index(name=\"order_count_month\")\n",
    "\n",
    "# Pivot table for last 3 months\n",
    "# (This assumes you have filtered dataset up to current month)\n",
    "monthly_pivot = monthly_orders.pivot(index=\"customer_id\", columns=\"order_month\", values=\"order_count_month\").fillna(0)\n",
    "\n",
    "# Example: if we label last 3 months as -1, -2, -3\n",
    "monthly_pivot[\"avg_freq_month_minus2_3\"] = monthly_pivot.iloc[:, -3:-1].mean(axis=1)  # baseline frequency\n",
    "monthly_pivot[\"avg_freq_last30\"] = monthly_pivot.iloc[:, -1]  # most recent month\n",
    "\n",
    "# Merge back into main dataframe\n",
    "df = df.merge(monthly_pivot[[\"avg_freq_month_minus2_3\", \"avg_freq_last30\"]], on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# Optional: create delta feature\n",
    "df[\"freq_change\"] = df[\"avg_freq_last30\"] - df[\"avg_freq_month_minus2_3\"]\n",
    "\n",
    "\n",
    "# Example engineered features\n",
    "df[\"recency_days\"] = (pd.to_datetime(\"today\") - df[\"last_order_date\"]).dt.days\n",
    "df[\"complaint_ratio\"] = df[\"complaint_count\"] / (df[\"order_count\"] + 1)\n",
    "df[\"refund_ratio\"] = df[\"refund_count\"] / (df[\"order_count\"] + 1)\n",
    "df[\"missing_ratio\"] = df[\"missing_items\"] / (df[\"order_count\"] + 1)\n",
    "df[\"fresh_ratio\"] = df[\"fresh_items\"] / (df[\"total_items\"] + 1)\n",
    "\n",
    "# Select features\n",
    "feature_cols = [\"recency_days\", \"order_count\", \"avg_order_value\",\n",
    "                \"complaint_ratio\", \"refund_ratio\", \"missing_ratio\", \"fresh_ratio\"]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[\"churn\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800e12b",
   "metadata": {},
   "source": [
    "## 4. Model Building & Training\n",
    "- Logistic Regression (baseline)  \n",
    "- Random Forest, XGBoost  \n",
    "- Hyperparameter tuning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ebf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "print(\"XGBoost\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8657d3c",
   "metadata": {},
   "source": [
    "## 5. Results & Evaluation\n",
    "- Compare Accuracy, F1-score, ROC-AUC  \n",
    "- Confusion matrix  \n",
    "- ROC curve  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"LR\": lr, \"RF\": rf, \"XGB\": xgb}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{name} - Classification Report\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # ROC\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"{name} - ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model (e.g. XGB)\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - XGBoost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ce2f6",
   "metadata": {},
   "source": [
    "## 6. Discussion & Conclusion\n",
    "- Best model: [fill in here]  \n",
    "- Key findings and interpretation  \n",
    "- Limitations of the current approach  \n",
    "- Future improvements (e.g., more features, deep learning, deployment)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9926e36",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Feature Engineering (S∆° b·ªô)\n",
    "\n",
    "- Merge d·ªØ li·ªáu t·ª´ **Orders**, **Customers**, **Incidents**, **Ratings**.\n",
    "- T·∫°o c√°c feature c∆° b·∫£n:\n",
    "  - `total_orders`\n",
    "  - `avg_rating`\n",
    "  - `num_incidents`\n",
    "  - `days_since_last_order`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31032c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "df_orders = pd.read_csv('Orders.csv')\n",
    "df_customers = pd.read_csv('Customers.csv')\n",
    "df_products = pd.read_csv('Products.csv')\n",
    "df_incidents = pd.read_csv('Incidents.csv')\n",
    "df_ratings = pd.read_csv('Order_rating.csv')\n",
    "\n",
    "# Merge c∆° b·∫£n\n",
    "df = df_orders.merge(df_customers, on='customer_id', how='left')\n",
    "df = df.merge(df_ratings, on='order_id', how='left')\n",
    "df = df.merge(df_incidents.groupby('customer_id').agg(num_incidents=('incident_id','count')).reset_index(),\n",
    "              on='customer_id', how='left')\n",
    "df['total_orders'] = df.groupby('customer_id')['order_id'].transform('count')\n",
    "df['avg_rating'] = df.groupby('customer_id')['rating'].transform('mean')\n",
    "df['days_since_last_order'] = (pd.to_datetime('today') - pd.to_datetime(df['order_date'])).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe234cdd",
   "metadata": {},
   "source": [
    "## üîç EDA ƒë·ªãnh h∆∞·ªõng m·ª•c ti√™u\n",
    "\n",
    "- So s√°nh churn rate theo c√°c feature s∆° b·ªô.\n",
    "- T√¨m ra y·∫øu t·ªë quan tr·ªçng ·∫£nh h∆∞·ªüng ƒë·∫øn churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d54d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Gi·∫£ s·ª≠ ƒë√£ c√≥ c·ªôt churn (1: churn, 0: active)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.boxplot(x='churn', y='total_orders', data=df, ax=axes[0])\n",
    "axes[0].set_title('Total Orders vs Churn')\n",
    "\n",
    "sns.boxplot(x='churn', y='avg_rating', data=df, ax=axes[1])\n",
    "axes[1].set_title('Avg Rating vs Churn')\n",
    "\n",
    "sns.boxplot(x='churn', y='num_incidents', data=df, ax=axes[2])\n",
    "axes[2].set_title('Num Incidents vs Churn')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9231cf",
   "metadata": {},
   "source": [
    "## üöÄ Feature Engineering (N√¢ng cao)\n",
    "\n",
    "D·ª±a tr√™n insight t·ª´ EDA:\n",
    "- T·∫°o feature rolling 30 ng√†y (`orders_last_30d`).\n",
    "- T√≠nh t·ª∑ l·ªá incidents ch∆∞a gi·∫£i quy·∫øt (`incident_unresolved_ratio`).\n",
    "- T√≠nh **diversity index** s·ªë l∆∞·ª£ng s·∫£n ph·∫©m kh√°c nhau kh√°ch h√†ng mua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33199c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling 30 ng√†y\n",
    "df_orders['order_date'] = pd.to_datetime(df_orders['order_date'])\n",
    "df_orders = df_orders.sort_values(['customer_id','order_date'])\n",
    "\n",
    "window = pd.Timedelta(days=30)\n",
    "df_orders['orders_last_30d'] = df_orders.groupby('customer_id')['order_date'].transform(\n",
    "    lambda x: x.rolling(window=30, min_periods=1).count()\n",
    ")\n",
    "\n",
    "# T·ª∑ l·ªá incidents ch∆∞a gi·∫£i quy·∫øt\n",
    "df_incidents['unresolved'] = df_incidents['status'].apply(lambda x: 1 if x!='resolved' else 0)\n",
    "incident_ratio = df_incidents.groupby('customer_id').agg(\n",
    "    incident_unresolved_ratio=('unresolved','mean')\n",
    ").reset_index()\n",
    "df = df.merge(incident_ratio, on='customer_id', how='left')\n",
    "\n",
    "# Diversity index s·∫£n ph·∫©m\n",
    "diversity = df_orders.groupby('customer_id')['product_id'].nunique().reset_index()\n",
    "diversity.rename(columns={'product_id':'diversity_index'}, inplace=True)\n",
    "df = df.merge(diversity, on='customer_id', how='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
