{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae908cb2",
   "metadata": {},
   "source": [
    "# ðŸ“Š Final Project - Customer Churn Prediction for Sendo Farm\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "Sendo Farm is an online grocery e-commerce platform that delivers essential goods, similar to a supermarket, directly to consumers for their daily meals.  \n",
    "Because the products are groceriesâ€”especially fresh items such as meat, fish, fruits, and vegetablesâ€”customer satisfaction is highly sensitive to quality.  \n",
    "\n",
    "Some customers who are dissatisfied with product quality or after-sales service choose to file complaints, request refunds, or report missing items. However, many others leave silently without expressing dissatisfaction and never purchase from Sendo Farm again.  \n",
    "\n",
    "Customer churn is costly, especially in the grocery business where customers make frequent and recurring purchases.  \n",
    "Thus, the goal of this project is to build a **supervised machine learning model** that predicts which customers are at risk of churning. This will allow Sendo Farm to proactively take preventive actions, such as personalized campaigns, compensation, or loyalty offers, to improve customer experience and retention.  \n",
    "\n",
    "### Dataset Description\n",
    "The available data includes:\n",
    "- **Transaction history** (order frequency, order value, recency of last purchase).\n",
    "- **Complaint & refund history** (number of complaints, refund requests).\n",
    "- **Missing items history** (number of orders with missing products).\n",
    "- **Purchase ratio of dry vs fresh goods** (stability and sensitivity to product type).\n",
    "- **Order rating** (order satisfaction)      \n",
    "\n",
    "These features will be engineered into customer-level data suitable for supervised ML classification (churn vs non-churn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba669a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edf916",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Initial Merge\n",
    "\n",
    "We will load all raw datasets (Orders, Customers, Incidents, Order_rating) and perform a preliminary merge to create a single customer-level dataset. This initial step simplifies subsequent feature engineering and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a8a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Data Sources - Let's use the provided mock dataframes for demonstration\n",
    "orders = pd.DataFrame({\n",
    "    \"order_id\": range(1, 11),\n",
    "    \"customer_id\": [1,1,2,3,3,3,4,5,5,6],\n",
    "    \"product_id\": [101,102,103,104,105,106,107,108,109,110],\n",
    "    \"order_date\": pd.to_datetime(pd.date_range(\"2024-01-01\", periods=10, freq=\"15D\")),\n",
    "    \"quantity\": np.random.randint(1,5, size=10),\n",
    "    \"price\": np.random.randint(50,200, size=10)\n",
    "})\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    \"customer_id\": [1,2,3,4,5,6],\n",
    "    \"gender\": [\"M\",\"F\",\"F\",\"M\",\"F\",\"M\"],\n",
    "    \"age\": [25,30,40,35,28,45],\n",
    "    \"region\": [\"North\",\"South\",\"South\",\"East\",\"West\",\"East\"],\n",
    "    \"join_date\": pd.to_datetime(pd.date_range(\"2023-01-01\", periods=6, freq=\"90D\"))\n",
    "})\n",
    "\n",
    "incidents = pd.DataFrame({\n",
    "    \"incident_id\": range(1,6),\n",
    "    \"order_id\": [2,4,6,8,9],\n",
    "    \"type\": [\"Refund\",\"Late Delivery\",\"Missing Item\",\"Refund\",\"Damaged\"],\n",
    "    \"date\": pd.to_datetime(pd.date_range(\"2024-02-01\", periods=5, freq=\"30D\")),\n",
    "    \"resolved_flag\": [1,0,1,1,0]\n",
    "})\n",
    "\n",
    "order_rating = pd.DataFrame({\n",
    "    \"order_id\": [1,2,3,5,7,9],\n",
    "    \"rating\": [5,3,4,2,5,1],\n",
    "    \"feedback_text\": [\"Good\",\"Late delivery\",\"Nice\",\"Bad packaging\",\"Excellent\",\"Broken item\"],\n",
    "    \"review_date\": pd.to_datetime(pd.date_range(\"2024-02-01\", periods=6, freq=\"20D\"))\n",
    "})\n",
    "\n",
    "# Merge orders with customer info\n",
    "df_merged = orders.merge(customers, on='customer_id', how='left')\n",
    "\n",
    "# Merge incidents and ratings via order_id\n",
    "df_merged = df_merged.merge(incidents, on='order_id', how='left')\n",
    "df_merged = df_merged.merge(order_rating, on='order_id', how='left')\n",
    "\n",
    "print(\"Initial Merged Dataset Head:\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616663a",
   "metadata": {},
   "source": [
    "## 2. Preliminary Feature Engineering\n",
    "\n",
    "Before diving deep into analysis, we'll create some essential customer-level features by aggregating the merged data. This gives us a basic dataset for initial exploration.\n",
    "\n",
    "### Feature Aggregation\n",
    "- **Total Orders**: Total number of orders per customer.\n",
    "- **Total Spend**: Sum of money spent by each customer.\n",
    "- **Avg Rating**: Average rating given by the customer.\n",
    "- **Num Incidents**: Total number of incidents (complaints, refunds) reported by a customer.\n",
    "- **Recency**: Number of days since the customer's last order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate features at the customer level\n",
    "customer_features = df_merged.groupby('customer_id').agg(\n",
    "    total_orders=('order_id', 'nunique'),\n",
    "    total_spend=('price', 'sum'),\n",
    "    avg_rating=('rating', 'mean'),\n",
    "    num_incidents=('incident_id', 'nunique'),\n",
    "    last_order_date=('order_date', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate Recency: days since the last order\n",
    "last_date = pd.to_datetime('today')\n",
    "customer_features['days_since_last_order'] = (last_date - customer_features['last_order_date']).dt.days\n",
    "\n",
    "# Merge with customer demographics\n",
    "customer_features = customer_features.merge(customers, on='customer_id', how='left')\n",
    "\n",
    "print(\"Customer-level Feature Dataset Head:\")\n",
    "print(customer_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f78dc",
   "metadata": {},
   "source": [
    "## 3. Targeted Exploratory Data Analysis (EDA)\n",
    "\n",
    "With our preliminary features, we can now conduct a targeted EDA to understand the relationship between these features and customer churn. The goal is to identify potential churn drivers and generate hypotheses for more advanced feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df50e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Define Churn based on Recency\n",
    "# Assuming a simple rule: churn if a customer has not ordered in the last 30 days.\n",
    "customer_features['churn'] = (customer_features['days_since_last_order'] > 30).astype(int)\n",
    "\n",
    "# Check Churn Distribution\n",
    "churn_dist = customer_features['churn'].value_counts(normalize=True)\n",
    "print(\"\\nChurn Distribution:\")\n",
    "print(churn_dist)\n",
    "churn_dist.plot(kind='bar', title='Churn vs Non-Churn Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Analyze Churn Rate vs Key Features\n",
    "print(\"\\nChurn rate by number of incidents:\")\n",
    "print(customer_features.groupby('num_incidents')['churn'].mean())\n",
    "\n",
    "print(\"\\nChurn rate by average rating:\")\n",
    "bins = [1, 2, 3, 4, 5]\n",
    "customer_features['avg_rating_bin'] = pd.cut(customer_features['avg_rating'], bins=bins, labels=False)\n",
    "print(customer_features.groupby('avg_rating_bin')['churn'].mean())\n",
    "\n",
    "# Visualize relationships\n",
    "sns.boxplot(x='churn', y='days_since_last_order', data=customer_features)\n",
    "plt.title('Days Since Last Order vs Churn')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x='num_incidents', y='churn', data=customer_features, estimator=np.mean)\n",
    "plt.title('Churn Rate vs Number of Incidents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a483174",
   "metadata": {},
   "source": [
    "## 4. Advanced Feature Engineering\n",
    "\n",
    "Based on the EDA findings, we will now engineer more sophisticated features that capture behavioral patterns over time, such as changes in order frequency. These features are often more predictive of churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6961c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Feature: Order Frequency Change\n",
    "# This feature captures a sudden drop in a customer's purchasing frequency.\n",
    "\n",
    "# Sort data by customer and date for time-series analysis\n",
    "orders_sorted = orders.sort_values(by=['customer_id', 'order_date'])\n",
    "\n",
    "# Calculate rolling order frequency (e.g., last 30 days)\n",
    "orders_sorted['rolling_30d_freq'] = orders_sorted.groupby('customer_id')['order_date'].rolling('30D').count().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate order frequency for specific periods (e.g., month -1, month -2)\n",
    "current_month = orders_sorted['order_date'].max().to_period('M')\n",
    "\n",
    "# Filter orders from the last two months\n",
    "orders_last_2_months = orders_sorted[orders_sorted['order_date'] >= (current_month - 1).to_timestamp()]\n",
    "\n",
    "monthly_orders = orders_last_2_months.groupby(['customer_id', orders_last_2_months['order_date'].dt.to_period('M')]).size().unstack(fill_value=0)\n",
    "\n",
    "# Create frequency change feature\n",
    "if monthly_orders.shape[1] >= 2:\n",
    "    monthly_orders['freq_month_minus1'] = monthly_orders.iloc[:, -1]\n",
    "    monthly_orders['freq_month_minus2'] = monthly_orders.iloc[:, -2]\n",
    "    monthly_orders['freq_change'] = monthly_orders['freq_month_minus1'] - monthly_orders['freq_month_minus2']\n",
    "    customer_features = customer_features.merge(monthly_orders[['freq_change']], on='customer_id', how='left')\n",
    "else:\n",
    "    print(\"Not enough data to create frequency change feature.\")\n",
    "    customer_features['freq_change'] = 0\n",
    "    \n",
    "print(\"\\nUpdated Customer Features with Advanced Features:\")\n",
    "print(customer_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f279e1a",
   "metadata": {},
   "source": [
    "## 5. Model Building & Training\n",
    "\n",
    "Now we have a rich feature set, we can train and evaluate different supervised machine learning models to predict churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, XGBClassifier\n",
    "\n",
    "# Handle potential missing values from merge\n",
    "customer_features.fillna(0, inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "feature_cols = ['total_orders', 'total_spend', 'avg_rating', 'num_incidents', 'days_since_last_order', 'freq_change']\n",
    "\n",
    "X = customer_features[feature_cols]\n",
    "y = customer_features['churn']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{name} - Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"{name} - ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481bd4a",
   "metadata": {},
   "source": [
    "## 6. Conclusion & Future Work\n",
    "\n",
    "Based on the evaluation metrics, we can determine the best-performing model. This project serves as a strong foundation, and future improvements could include:\n",
    "\n",
    "- **Feature Scaling**: Apply `StandardScaler` to numerical features for models like Logistic Regression.\n",
    "- **Hyperparameter Tuning**: Use `GridSearchCV` or `RandomizedSearchCV` to find optimal parameters for models like Random Forest and XGBoost.\n",
    "- **Advanced Features**: Incorporate more features from the `Products` table (e.g., proportion of fresh vs. dry goods purchased).\n",
    "- **Deployment**: Integrate the best model into a real-time system to identify at-risk customers dynamically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
