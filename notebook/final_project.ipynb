{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae908cb2",
   "metadata": {},
   "source": [
    "# üìä Final Project - Customer Churn Prediction for Sendo Farm\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "Sendo Farm is an online grocery e-commerce platform that delivers essential goods, similar to a supermarket, directly to consumers for their daily meals.  \n",
    "Because the products are groceries‚Äîespecially fresh items such as meat, fish, fruits, and vegetables‚Äîcustomer satisfaction is highly sensitive to quality.  \n",
    "\n",
    "Some customers who are dissatisfied with product quality or after-sales service choose to file complaints, request refunds, or report missing items. However, many others leave silently without expressing dissatisfaction and never purchase from Sendo Farm again.  \n",
    "\n",
    "Customer churn is costly, especially in the grocery business where customers make frequent and recurring purchases.  \n",
    "Thus, the goal of this project is to build a **supervised machine learning model** that predicts which customers are at risk of churning. This will allow Sendo Farm to proactively take preventive actions, such as personalized campaigns, compensation, or loyalty offers, to improve customer experience and retention.  \n",
    "\n",
    "### Dataset Description\n",
    "The available data includes:\n",
    "- **Customer infomation** (customerid, monthly_frequency, recency, days_stop_frequency, total_value, last_rating, avarage_rating, last_complain, last_refund, total_comlains, total_refunds) \n",
    "- **Transaction history** (customerid, orderid, orderdate,rating, is_compalained, order_value).\n",
    "     \n",
    "\n",
    "These features will be engineered into customer-level data suitable for supervised ML classification (churn vs non-churn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba669a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edf916",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Initial Merge\n",
    "\n",
    "### Overview\n",
    "This section focuses on loading and reviewing transaction data from the grocery_transactions.csv file, specifically targeting transactions from **June, July, and August 2025** for **10,000 customers**. The primary objectives are:\n",
    "\n",
    "1. **Load the dataset** and perform initial data exploration\n",
    "2. **Review data quality** and structure\n",
    "\n",
    "3. **The printed out information as below:**\n",
    "\n",
    "Total rows (transactions): 77,618\n",
    "\n",
    "Unique customer IDs: 10,000\n",
    "\n",
    "Data columns (total 9 columns):so.ordernumber, orderdate, order_value, customerid, applyclaimdate, refundguid, reasoncancelcode, rating, is_fresh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24a8a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows (transactions): 77,618\n",
      "Unique customer IDs: 10,000\n",
      "Column Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77618 entries, 0 to 77617\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   so.ordernumber    77618 non-null  int64  \n",
      " 1   orderdate         77618 non-null  object \n",
      " 2   order_value       77618 non-null  float64\n",
      " 3   customerid        77618 non-null  int64  \n",
      " 4   applyclaimdate    560 non-null    object \n",
      " 5   refundguid        1161 non-null   object \n",
      " 6   reasoncancelcode  4475 non-null   object \n",
      " 7   rating            10261 non-null  float64\n",
      " 8   is_fresh          77618 non-null  int64  \n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 5.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# üìä Load grocery transactions data\n",
    "df = pd.read_csv('../data/grocery_transactions.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Total rows (transactions): {len(df):,}\")\n",
    "#Print unique customerid here \n",
    "print(f\"Unique customer IDs: {df['customerid'].nunique():,}\")\n",
    "# Display column information\n",
    "print(\"Column Information:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616663a",
   "metadata": {},
   "source": [
    "## 2. Preliminary Feature Engineering\n",
    "\n",
    "Before diving deep into analysis, we'll create some essential customer-level features by aggregating the merged data. This gives us a basic dataset for initial exploration.\n",
    "\n",
    "### Feature Aggregation\n",
    "- **customerid**: Customer level group by (10K)\n",
    "- **total_order**: Total number of orders per customer (count from ordernumber).\n",
    "- **total_spend**: Sum of money spent by each customer (sum from order_value).\n",
    "- **is_bad_rating**: Normalize rating given by the customer (If average rating < 3.5 then 1 else 0).\n",
    "- **is_bad_last_rating**: Normalize last rating given by the customer (If most recent rating < 3.5 then 1 else 0).\n",
    "- **total_incidents**: Total number of incidents (refunds, incidents) by a customer (count(refundguid) + count(applyclaimdate) ).\n",
    "- **is_incident_last_order**: Was there an incident at last order (If last order is incident or refurnd then 1 else 0).\n",
    "- **is_fresh_last_order**: Is there any fresh item in last order (If last order having is_fresh = true then 1 else 0). \n",
    "- **is_canceled_last order** Was the last order  cancelled? (If reasoncancelcode of last order is NOT NULL then 1 else 0 ).\n",
    "- **Recency**: Number of days since the customer's last order ( Day('08-31-2925' - max(orderdate))).\n",
    "- **is_churn**: This is the label column (If Recency > 15 then 1 else 0).\n",
    "\n",
    "Printed out the head and dataset info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cff1c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating customer-level features...\n",
      "==================================================\n",
      "Calculating average rating for orders with actual ratings...\n",
      "Calculating derived features...\n",
      "‚úÖ Customer-level features created successfully!\n",
      "üìä Dataset shape: (10000, 17)\n",
      "\n",
      "üîç Customer Features Dataset Information:\n",
      "==================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 17 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   customerid              10000 non-null  int64         \n",
      " 1   total_order             10000 non-null  int64         \n",
      " 2   total_spend             10000 non-null  float64       \n",
      " 3   last_rating             10000 non-null  float64       \n",
      " 4   total_refunds           10000 non-null  int64         \n",
      " 5   total_claims            10000 non-null  int64         \n",
      " 6   total_cancellations     10000 non-null  int64         \n",
      " 7   is_fresh_last_order     10000 non-null  int64         \n",
      " 8   last_order_date         10000 non-null  datetime64[ns]\n",
      " 9   avg_rating              1500 non-null   float64       \n",
      " 10  is_bad_rating           10000 non-null  int64         \n",
      " 11  is_bad_last_rating      10000 non-null  int64         \n",
      " 12  total_incidents         10000 non-null  int64         \n",
      " 13  is_incident_last_order  10000 non-null  int64         \n",
      " 14  is_canceled_last_order  10000 non-null  int64         \n",
      " 15  recency                 10000 non-null  int64         \n",
      " 16  is_churn                10000 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(3), int64(13)\n",
      "memory usage: 1.3 MB\n",
      "None\n",
      "\n",
      "üëÄ First 10 rows of customer features:\n",
      "==================================================\n",
      "   customerid  total_order  total_spend  last_rating  total_refunds  \\\n",
      "0  1010090733            4    2594900.0          5.0              0   \n",
      "1  1010106706            2      24800.0          4.0              0   \n",
      "2  1058107719           17    3336400.0          0.0              0   \n",
      "3  1064092966            4     588600.0          0.0              0   \n",
      "4  1074092273            4    1116700.0          0.0              0   \n",
      "5  1076084447           12    1766400.0          0.0              0   \n",
      "6  1078110127           39     716694.0          1.0              0   \n",
      "7  1096081809            1      37900.0          0.0              0   \n",
      "8  1110092581            9    3656300.0          0.0              0   \n",
      "9  1150104568           10    1187700.0          0.0              0   \n",
      "\n",
      "   total_claims  total_cancellations  is_fresh_last_order     last_order_date  \\\n",
      "0             0                    0                    1 2025-08-20 12:09:06   \n",
      "1             0                    0                    1 2025-07-17 16:56:22   \n",
      "2             0                    0                    1 2025-08-24 11:32:26   \n",
      "3             0                    0                    0 2025-06-24 17:48:46   \n",
      "4             0                    0                    1 2025-08-27 14:01:15   \n",
      "5             0                    0                    1 2025-08-22 09:37:08   \n",
      "6             0                    0                    1 2025-08-24 20:36:06   \n",
      "7             0                    0                    0 2025-08-16 15:15:25   \n",
      "8             0                    0                    1 2025-08-08 10:52:18   \n",
      "9             0                    0                    1 2025-08-20 19:36:29   \n",
      "\n",
      "   avg_rating  is_bad_rating  is_bad_last_rating  total_incidents  \\\n",
      "0         5.0              0                   0                0   \n",
      "1         4.0              0                   0                0   \n",
      "2         NaN              0                   0                0   \n",
      "3         NaN              0                   0                0   \n",
      "4         NaN              0                   0                0   \n",
      "5         NaN              0                   0                0   \n",
      "6         1.0              1                   1                0   \n",
      "7         NaN              0                   0                0   \n",
      "8         NaN              0                   0                0   \n",
      "9         NaN              0                   0                0   \n",
      "\n",
      "   is_incident_last_order  is_canceled_last_order  recency  is_churn  \n",
      "0                       0                       0       10         0  \n",
      "1                       0                       0       44         1  \n",
      "2                       0                       0        6         0  \n",
      "3                       0                       0       67         1  \n",
      "4                       0                       0        3         0  \n",
      "5                       0                       0        8         0  \n",
      "6                       0                       0        6         0  \n",
      "7                       0                       0       14         0  \n",
      "8                       0                       0       22         1  \n",
      "9                       0                       0       10         0  \n",
      "\n",
      "üìä Basic Statistics:\n",
      "==================================================\n",
      "         customerid   total_order   total_spend  last_rating  total_refunds  \\\n",
      "count  1.000000e+04  10000.000000  1.000000e+04  10000.00000   10000.000000   \n",
      "mean   2.036832e+09      7.761800  1.275983e+06      0.61530       0.116100   \n",
      "min    1.010091e+09      1.000000  0.000000e+00      0.00000       0.000000   \n",
      "25%    2.039108e+09      1.000000  1.154750e+05      0.00000       0.000000   \n",
      "50%    2.042727e+09      3.000000  3.348500e+05      0.00000       0.000000   \n",
      "75%    2.043427e+09      7.000000  1.038950e+06      0.00000       0.000000   \n",
      "max    2.043504e+09    571.000000  2.366864e+08      5.00000      16.000000   \n",
      "std    4.017987e+07     17.123299  4.965457e+06      1.53634       0.564316   \n",
      "\n",
      "       total_claims  total_cancellations  is_fresh_last_order  \\\n",
      "count  10000.000000         10000.000000         10000.000000   \n",
      "mean       0.056000             0.447500             0.575100   \n",
      "min        0.000000             0.000000             0.000000   \n",
      "25%        0.000000             0.000000             0.000000   \n",
      "50%        0.000000             0.000000             1.000000   \n",
      "75%        0.000000             0.000000             1.000000   \n",
      "max        9.000000            25.000000             1.000000   \n",
      "std        0.321363             1.283982             0.494353   \n",
      "\n",
      "                     last_order_date   avg_rating  is_bad_rating  \\\n",
      "count                          10000  1500.000000   10000.000000   \n",
      "mean   2025-07-29 10:57:11.402500096     4.117137       0.029000   \n",
      "min              2025-06-01 00:14:35     1.000000       0.000000   \n",
      "25%    2025-07-03 13:34:31.249999872     4.000000       0.000000   \n",
      "50%              2025-08-05 16:22:12     4.500000       0.000000   \n",
      "75%    2025-08-24 16:57:32.249999872     5.000000       0.000000   \n",
      "max              2025-08-30 23:58:46     5.000000       1.000000   \n",
      "std                              NaN     1.140760       0.167815   \n",
      "\n",
      "       is_bad_last_rating  total_incidents  is_incident_last_order  \\\n",
      "count        10000.000000     10000.000000            10000.000000   \n",
      "mean             0.030200         0.172100                0.010800   \n",
      "min              0.000000         0.000000                0.000000   \n",
      "25%              0.000000         0.000000                0.000000   \n",
      "50%              0.000000         0.000000                0.000000   \n",
      "75%              0.000000         0.000000                0.000000   \n",
      "max              1.000000        20.000000                1.000000   \n",
      "std              0.171146         0.791798                0.103366   \n",
      "\n",
      "       is_canceled_last_order      recency      is_churn  \n",
      "count            10000.000000  10000.00000  10000.000000  \n",
      "mean                 0.053300     32.22750      0.594300  \n",
      "min                  0.000000      0.00000      0.000000  \n",
      "25%                  0.000000      6.00000      0.000000  \n",
      "50%                  0.000000     25.00000      1.000000  \n",
      "75%                  0.000000     58.00000      1.000000  \n",
      "max                  1.000000     90.00000      1.000000  \n",
      "std                  0.224642     27.52188      0.491052  \n",
      "\n",
      "üéØ Churn Distribution:\n",
      "==================================================\n",
      "Non-churn customers (0): 4,057 (40.6%)\n",
      "Churn customers (1): 5,943 (59.4%)\n",
      "\n",
      "üìà Feature Summary:\n",
      "==================================================\n",
      "‚Ä¢ Total customers: 10,000\n",
      "‚Ä¢ Average orders per customer: 7.76\n",
      "‚Ä¢ Average spend per customer: $1275983.20\n",
      "‚Ä¢ Average rating (customers with ratings): 4.12\n",
      "‚Ä¢ Customers with ratings: 1,500\n",
      "‚Ä¢ Customers with bad ratings: 290\n",
      "‚Ä¢ Customers with incidents: 1,721\n",
      "‚Ä¢ Average recency: 32.2 days\n",
      "\n",
      "‚úÖ Preliminary feature engineering completed!\n"
     ]
    }
   ],
   "source": [
    "# üîß Create customer-level features by aggregating transaction data\n",
    "print(\"Creating customer-level features...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Ensure orderdate is datetime\n",
    "df['orderdate'] = pd.to_datetime(df['orderdate'])\n",
    "\n",
    "# Group by customerid and create aggregated features\n",
    "customer_features = df.groupby('customerid').agg({\n",
    "    'so.ordernumber': 'count',  # total_order\n",
    "    'order_value': 'sum',       # total_spend\n",
    "    'rating': 'last',           # last_rating only\n",
    "    'refundguid': 'count',      # count refunds\n",
    "    'applyclaimdate': 'count',  # count claims\n",
    "    'reasoncancelcode': 'count', # count cancellations\n",
    "    'is_fresh': 'last',         # is_fresh_last_order\n",
    "    'orderdate': 'max'          # last_order_date\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "customer_features.columns = [\n",
    "    'customerid', 'total_order', 'total_spend', 'last_rating',\n",
    "    'total_refunds', 'total_claims', 'total_cancellations', 'is_fresh_last_order', 'last_order_date'\n",
    "]\n",
    "\n",
    "# Calculate avg_rating separately - only for orders with actual ratings (not NULL)\n",
    "print(\"Calculating average rating for orders with actual ratings...\")\n",
    "avg_rating_by_customer = df[df['rating'].notna()].groupby('customerid')['rating'].mean()\n",
    "customer_features['avg_rating'] = customer_features['customerid'].map(avg_rating_by_customer)\n",
    "\n",
    "# Calculate additional features\n",
    "print(\"Calculating derived features...\")\n",
    "\n",
    "# 1. is_bad_rating: If average rating < 3.5 then 1 else 0 (only for customers with ratings)\n",
    "customer_features['is_bad_rating'] = (customer_features['avg_rating'] < 3.5).astype(int)\n",
    "# For customers with no ratings, is_bad_rating = 0 (neutral)\n",
    "customer_features['is_bad_rating'] = customer_features['is_bad_rating'].fillna(0)\n",
    "\n",
    "# 2. is_bad_last_rating: If most recent rating < 3.5 then 1 else 0\n",
    "customer_features['is_bad_last_rating'] = (customer_features['last_rating'] < 3.5).astype(int)\n",
    "\n",
    "# 3. total_incidents: Total number of incidents (refunds + claims)\n",
    "customer_features['total_incidents'] = customer_features['total_refunds'] + customer_features['total_claims']\n",
    "\n",
    "# 4. is_incident_last_order: Was there an incident at last order\n",
    "# We need to check if the last order had an incident or refund\n",
    "last_orders = df.sort_values('orderdate').groupby('customerid').tail(1)\n",
    "last_orders_incidents = last_orders.set_index('customerid')[['refundguid', 'applyclaimdate']].notna().any(axis=1)\n",
    "customer_features['is_incident_last_order'] = customer_features['customerid'].map(last_orders_incidents).fillna(0).astype(int)\n",
    "\n",
    "# 5. is_canceled_last_order: Was the last order cancelled?\n",
    "last_orders_canceled = last_orders.set_index('customerid')['reasoncancelcode'].notna()\n",
    "customer_features['is_canceled_last_order'] = customer_features['customerid'].map(last_orders_canceled).fillna(0).astype(int)\n",
    "\n",
    "# 6. Recency: Number of days since the customer's last order\n",
    "reference_date = pd.to_datetime('2025-08-31')  # End of August 2025\n",
    "customer_features['recency'] = (reference_date - customer_features['last_order_date']).dt.days\n",
    "\n",
    "# 7. is_churn: If Recency > 15 then 1 else 0\n",
    "customer_features['is_churn'] = (customer_features['recency'] > 15).astype(int)\n",
    "\n",
    "# Handle missing values\n",
    "customer_features['last_rating'] = customer_features['last_rating'].fillna(0)\n",
    "# avg_rating remains NaN for customers with no ratings (this is correct behavior)\n",
    "\n",
    "print(\"‚úÖ Customer-level features created successfully!\")\n",
    "print(f\"üìä Dataset shape: {customer_features.shape}\")\n",
    "print()\n",
    "\n",
    "# Display basic information\n",
    "print(\"üîç Customer Features Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "print(customer_features.info())\n",
    "print()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"üëÄ First 10 rows of customer features:\")\n",
    "print(\"=\" * 50)\n",
    "print(customer_features.head(10))\n",
    "print()\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"üìä Basic Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(customer_features.describe())\n",
    "print()\n",
    "\n",
    "# Display churn distribution\n",
    "print(\"üéØ Churn Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "churn_counts = customer_features['is_churn'].value_counts()\n",
    "print(f\"Non-churn customers (0): {churn_counts[0]:,} ({churn_counts[0]/len(customer_features)*100:.1f}%)\")\n",
    "print(f\"Churn customers (1): {churn_counts[1]:,} ({churn_counts[1]/len(customer_features)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Display feature summary\n",
    "print(\"üìà Feature Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚Ä¢ Total customers: {len(customer_features):,}\")\n",
    "print(f\"‚Ä¢ Average orders per customer: {customer_features['total_order'].mean():.2f}\")\n",
    "print(f\"‚Ä¢ Average spend per customer: ${customer_features['total_spend'].mean():.2f}\")\n",
    "print(f\"‚Ä¢ Average rating (customers with ratings): {customer_features['avg_rating'].mean():.2f}\")\n",
    "print(f\"‚Ä¢ Customers with ratings: {customer_features['avg_rating'].notna().sum():,}\")\n",
    "print(f\"‚Ä¢ Customers with bad ratings: {customer_features['is_bad_rating'].sum():,}\")\n",
    "print(f\"‚Ä¢ Customers with incidents: {customer_features['total_incidents'].sum():,}\")\n",
    "print(f\"‚Ä¢ Average recency: {customer_features['recency'].mean():.1f} days\")\n",
    "\n",
    "print(\"\\n‚úÖ Preliminary feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f78dc",
   "metadata": {},
   "source": [
    "## 3. Targeted Exploratory Data Analysis (EDA)\n",
    "\n",
    "With our preliminary features, we can now conduct a targeted EDA to understand the relationship between these features and customer churn. The goal is to identify potential churn drivers and generate hypotheses for more advanced feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df50e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Define Churn based on Recency\n",
    "# Assuming a simple rule: churn if a customer has not ordered in the last 30 days.\n",
    "customer_features['churn'] = (customer_features['days_since_last_order'] > 30).astype(int)\n",
    "\n",
    "# Check Churn Distribution\n",
    "churn_dist = customer_features['churn'].value_counts(normalize=True)\n",
    "print(\"\\nChurn Distribution:\")\n",
    "print(churn_dist)\n",
    "churn_dist.plot(kind='bar', title='Churn vs Non-Churn Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Analyze Churn Rate vs Key Features\n",
    "print(\"\\nChurn rate by number of incidents:\")\n",
    "print(customer_features.groupby('num_incidents')['churn'].mean())\n",
    "\n",
    "print(\"\\nChurn rate by average rating:\")\n",
    "bins = [1, 2, 3, 4, 5]\n",
    "customer_features['avg_rating_bin'] = pd.cut(customer_features['avg_rating'], bins=bins, labels=False)\n",
    "print(customer_features.groupby('avg_rating_bin')['churn'].mean())\n",
    "\n",
    "# Visualize relationships\n",
    "sns.boxplot(x='churn', y='days_since_last_order', data=customer_features)\n",
    "plt.title('Days Since Last Order vs Churn')\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x='num_incidents', y='churn', data=customer_features, estimator=np.mean)\n",
    "plt.title('Churn Rate vs Number of Incidents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a483174",
   "metadata": {},
   "source": [
    "## 4. Advanced Feature Engineering\n",
    "\n",
    "Based on the EDA findings, we will now engineer more sophisticated features that capture behavioral patterns over time, such as changes in order frequency. These features are often more predictive of churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6961c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Feature: Order Frequency Change\n",
    "# This feature captures a sudden drop in a customer's purchasing frequency.\n",
    "\n",
    "# Sort data by customer and date for time-series analysis\n",
    "orders_sorted = orders.sort_values(by=['customer_id', 'order_date'])\n",
    "\n",
    "# Calculate rolling order frequency (e.g., last 30 days)\n",
    "orders_sorted['rolling_30d_freq'] = orders_sorted.groupby('customer_id')['order_date'].rolling('30D').count().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate order frequency for specific periods (e.g., month -1, month -2)\n",
    "current_month = orders_sorted['order_date'].max().to_period('M')\n",
    "\n",
    "# Filter orders from the last two months\n",
    "orders_last_2_months = orders_sorted[orders_sorted['order_date'] >= (current_month - 1).to_timestamp()]\n",
    "\n",
    "monthly_orders = orders_last_2_months.groupby(['customer_id', orders_last_2_months['order_date'].dt.to_period('M')]).size().unstack(fill_value=0)\n",
    "\n",
    "# Create frequency change feature\n",
    "if monthly_orders.shape[1] >= 2:\n",
    "    monthly_orders['freq_month_minus1'] = monthly_orders.iloc[:, -1]\n",
    "    monthly_orders['freq_month_minus2'] = monthly_orders.iloc[:, -2]\n",
    "    monthly_orders['freq_change'] = monthly_orders['freq_month_minus1'] - monthly_orders['freq_month_minus2']\n",
    "    customer_features = customer_features.merge(monthly_orders[['freq_change']], on='customer_id', how='left')\n",
    "else:\n",
    "    print(\"Not enough data to create frequency change feature.\")\n",
    "    customer_features['freq_change'] = 0\n",
    "    \n",
    "print(\"\\nUpdated Customer Features with Advanced Features:\")\n",
    "print(customer_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f279e1a",
   "metadata": {},
   "source": [
    "## 5. Model Building & Training\n",
    "\n",
    "Now we have a rich feature set, we can train and evaluate different supervised machine learning models to predict churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, XGBClassifier\n",
    "\n",
    "# Handle potential missing values from merge\n",
    "customer_features.fillna(0, inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "feature_cols = ['total_orders', 'total_spend', 'avg_rating', 'num_incidents', 'days_since_last_order', 'freq_change']\n",
    "\n",
    "X = customer_features[feature_cols]\n",
    "y = customer_features['churn']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{name} - Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"{name} - ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d57b5",
   "metadata": {},
   "source": [
    "## 6. Results & Evaluation\n",
    "- Compare Accuracy, F1-score, ROC-AUC  \n",
    "- Confusion matrix  \n",
    "- ROC curve  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"LR\": lr, \"RF\": rf, \"XGB\": xgb}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{name} - Classification Report\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # ROC\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"{name} - ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53721be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model (e.g. XGB)\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - XGBoost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481bd4a",
   "metadata": {},
   "source": [
    "## 6. Conclusion & Future Work\n",
    "\n",
    "Based on the evaluation metrics, we can determine the best-performing model. This project serves as a strong foundation, and future improvements could include:\n",
    "\n",
    "- **Feature Scaling**: Apply `StandardScaler` to numerical features for models like Logistic Regression.\n",
    "- **Hyperparameter Tuning**: Use `GridSearchCV` or `RandomizedSearchCV` to find optimal parameters for models like Random Forest and XGBoost.\n",
    "- **Advanced Features**: Incorporate more features from the `Products` table (e.g., proportion of fresh vs. dry goods purchased).\n",
    "- **Deployment**: Integrate the best model into a real-time system to identify at-risk customers dynamically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
